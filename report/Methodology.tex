\chapter{Methodology}\label{methodology}

While exploring the field in the previous chapter, the first research question was answered in the process.
The next three chapters will be used to answer the remaining questions using the following techniques, which are further explained in the sections below the list:

\begin{itemize}
	\item{Review of existing tools}
	\item{An adaptation of the 'design science' process}%?
	\item{An evaluation with test subjects}
\end{itemize}

\section{Tool review}\label{methReview}

Having looked into the various tools available, and compared their feature sets to determine which one is most likely to fit with the current teaching environment, question two is close to an answer as well.
By performing a closer study of the features of \gls{jive}, its compatibility with the teaching environment can be answered to a better degree.
Such a study is also an essential part in finding whether there is any room for improvement, and answering the third question.
In addition to discovering potential improvements through the use and exploration of \gls{jive}, determining whether the changes necessary to implement them are feasible to accomplish within the given time-frame is important to avoid half-implemented changes.


\section{Design science}\label{methDesign}

After thoroughly exploring \gls{jive}, and identifying potential improvements, it is time to select what to improve, and implement the necessary changes.
The available time-frame limits how many improvements it is possible to implement, as some time will most certainly be lost in understanding the source code of \gls{jive}.
Some time is also required at the end to answer the final research question.
This is best answered by performing a user evaluation with students in the target group.
This group consists of those described in the introduction, students in their second year of the computer science and informatics study-programs.

\section{Evaluation}\label{methEval}

In order to determine the usefulness of the selected tool, and the effect of the implemented changes, an evaluation with participants from the targeted user group is necessary.
The types of evaluations available are limited, with web-based questionnaires and in-person interviews being most suitable.

%web-based q
Reaching a wide audience, the use of web-based questionnaires can provide a large amount of data to support the research, but the nature of what is being evaluated makes this method unfit.
In order to give proper answers, the participants require a hands-on experience with the tool, and that would require them to acquire and install both the original version, and the modified version
This is a rather large obstacle that would deter most potential candidates from participating, and defeats the point of a questionnaire.
Although they could be tasked with comparing images of existing and new functionality, and given performance numbers to consider, it would not be the same as actually experiencing the differences, and getting to explore them at ones own pace.

Another issue appears in how questions are formulated, and what kind of answers they result in.
Requiring participants to quantify their experiences along numbered scales makes it easy to combine the results from all participants.
On the other hand, as each question must be limited in scope in order to get a precise answer, the amount of questions needed to get a good overview of each participants experience, can easily be perceived as different ways of asking the same question, resulting in repeated answers.

%interview
The alternative is an in-person evaluation with a smaller group of students, where the participants get access to a pre-configured system and enough time to make an informed opinion of what they are presented with.
As the availability of volunteers is limited, a properly executed in-person evaluation is likely to get more information out of the participants, by asking questions and discussing potential issues with them.

A hybrid approach could also be used by performing an in-person evaluation, and requiring participants to fill in a questionnaire afterwards.
This would still require one or more pre-configured systems, and some time spent on each participant to give an introduction to the tool they are evaluating.
Doing this in an effective manner, requires multiple assistants, and while the meaning of the questions could be further explained upon request, the answers would still be limited in the same manner as with a web-based questionnaire.


%design science, se p√• it3010 forskningsmetoder i informatikk