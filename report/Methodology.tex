\chapter{Methodology}\label{methodology}

While exploring the field in the previous chapter, the first research question was answered in the process.
The next three chapters will be used to answer the remaining questions using the following techniques, which are further elaborated on in the upcoming sections, which are:

\begin{itemize*}
	\item{Review of existing tools}
	\item{An adaptation of the `design science' process}%?
	\item{An evaluation with test subjects}
\end{itemize*}

\section{Tool review}\label{methReview}

Having looked into the various tools available, and compared their feature sets to determine which one is most likely to fit with the current teaching environment, the second question is close to an answer as well.
In order to determine whether \gls{jive} can be properly integrated with the teaching environment, a closer study is necessary.
This study will focus on the features of \gls{jive}, looking at what information they give the users.
By examining the features, a fair assumption is that some of the most apparent issues will be revealed, providing a partial answer to the third research question.

\section{Design science}\label{methDesign}

%find issues
%think of ideas
%determine possibility of implementation
%start implementation
	%encounter new issues/challenges with impl
	%rework solution
%done

When encountering an issue, its nature must be determined, identifying exactly what the problem is, and why it is a problem.
After figuring out these details, it becomes possible to identify ways to solve, or work around the problem.
These initial solutions must then be considered in light of the existing system, taking compatibility, complexity and effort into account.
The ideal solution may turn out to require a complete rewrite of the system, while a less optimal solution can be accomplished with minor modifications.
The desirable solution is, when considering the available time frame, one that requires a minimal modification of the existing system, but still results in a noticeably useful change.

After deciding that a feature is needed, work is started to find the best approach to implementing it.
This includes finding what parts of the code that are relevant, and needs to be modified.
Examining the existing code will give an overview of which parts of the information the feature requires is available, and in turn, narrows down the possible ways of implementing said feature.
This examination will also serve to get an idea of the effort required to implement a feature, determining which approach to use, and give a comparable value for the prioritizing of features.

\section{Evaluation}\label{methEval}

In order to determine the usefulness of the selected tool, and the effect of the implemented changes, an evaluation with participants from the targeted user group is necessary.
The types of evaluations available are limited, with web-based questionnaires and in-person interviews being most suitable.

%web-based q
Reaching a wide audience, the use of web-based questionnaires can provide a large amount of data to support the research, but the nature of what is being evaluated makes this method unfit.
In order to give proper answers, the participants require a hands-on experience with the tool, and that would require them to acquire and install both the original version, and the modified version
This is a rather large obstacle that would deter most potential candidates from participating, and defeats the point of a questionnaire.
Although they could be tasked with comparing images of existing and new functionality, and given performance numbers to consider, it would not be the same as actually experiencing the differences, and getting to explore them at the users own pace.

Another issue appears in how questions are formulated, and what kind of answers they result in.
Requiring participants to quantify their experiences along numbered scales makes it easy to combine the results from all participants.
On the other hand, as each question must be limited in scope in order to get a precise answer, the amount of questions needed to get a good overview of each participants experience, can easily be perceived as different ways of asking the same question, resulting in repeated answers.

%interview
The alternative is an in-person evaluation with a smaller group of students, where the participants get access to a pre-configured system and enough time to make an informed opinion of what they are presented with.
As the availability of volunteers is limited, a properly executed in-person evaluation is likely to get more information out of the participants, by asking questions and discussing potential issues with them.

A hybrid approach could also be used by performing an in-person evaluation, and requiring participants to fill in a questionnaire afterwards.
This would still require one or more pre-configured systems, and some time spent on each participant to give an introduction to the tool they are evaluating.
Doing this in an effective manner, requires multiple assistants, and while the meaning of the questions could be further explained upon request, the answers would still be limited in the same manner as with a web-based questionnaire.


%design science, se p√• it3010 forskningsmetoder i informatikk