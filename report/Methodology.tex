\chapter{Methodology}\label{methodology}%TODO: rewerite so all is design science adaptation?

While exploring the field in the previous chapter, the first research question was answered in the process.
The next three chapters will be used to answer the remaining questions using the following techniques, which are further elaborated on in the upcoming sections, which are:

\begin{itemize*}
	\item{Review of existing tools}
	\item{An adaptation of the `design science' process}%?
	\item{An evaluation with test subjects}
\end{itemize*}

\section{Tool review}\label{methReview}

Having looked into the various tools available, and compared their feature sets to determine which one is most likely to fit with the current teaching environment, the second question is close to an answer as well.
In order to determine whether \gls{jive} can be properly integrated with the teaching environment, a closer study is necessary.
This study will focus on the features of \gls{jive}, looking at what information they give the users.
By examining the features, a fair assumption is that some of the most apparent issues will be revealed, providing a partial answer to research question three.

\section{Design science}\label{methDesign}

%what is design science
As described in \cite{Vaishnavi2004}, design Science involves a five-step process consisting of the following phases: Awareness of the problem, suggestion, development, evaluation and conclusion.
These phases can be revisited during the process, resulting in an iterative cyclic process.
Repeating previous steps can often be necessary as more knowledge of a subject is acquired, and previous assumptions must be reconsidered.

The extended study of \gls{jive} can be considered to be a part of the first phase, as problems or issues are identified.
But as the intention is to discover most issues during the examination and then follow up with suggestions and possible implementations, it makes sense to consider this step as a separate process.
It is expected that the awareness-phase will be revisited during the development and implementation of features, triggered by the discovery of technical details that change the possibilities for a solution.

Following the discovery of issues, it is natural to do a collective suggestion-phase, as opposed to following the entire design science process for each issue.

From the development-phase and onwards, it becomes necessary to focus on one issue at a time.
During development, various challenges that require rethinking of the proposed solution are to be expected, causing several iterations of internal evaluation and a return to the suggestion-phase.

While each of the issues that result in an implemented change goes through internal evaluation during development, there is also need for external evaluation.
This is another phase where it is suitable to gather all issues, and the intended process is elaborated on in the following section.

%what did i do?

%what are the differences?

%why did i do so?

%find issues
%think of ideas
%determine possibility of implementation
%start implementation
	%encounter new issues/challenges with impl
	%rework solution
%done

\section{Evaluation}\label{methEval}

In order to determine the usefulness of the selected tool, and the effect of the implemented changes, an evaluation with participants from the targeted user group is necessary.
The types of evaluations available are limited, with web-based questionnaires and in-person interviews being most suitable.

%web-based q
Reaching a wide audience, the use of web-based questionnaires can provide a large amount of data to support the research, but the nature of what is being evaluated makes this method unfit.
In order to give proper answers, the participants require a hands-on experience with the tool, and that would require them to acquire and install both the original version, and the modified version
This is a rather large obstacle that would deter most potential candidates from participating, and defeats the point of a questionnaire.
Although they could be tasked with comparing images of existing and new functionality, and given performance numbers to consider, it would not be the same as actually experiencing the differences, and getting to explore them at the users own pace.

Another issue appears in how questions are formulated, and what kind of answers they result in.
Requiring participants to quantify their experiences along numbered scales makes it easy to combine the results from all participants.
On the other hand, as each question must be limited in scope in order to get a precise answer, the amount of questions needed to get a good overview of each participants experience, can easily be perceived as different ways of asking the same question, resulting in repeated answers.

%interview
The alternative is an in-person evaluation with a smaller group of students, where the participants get access to a pre-configured system and enough time to make an informed opinion of what they are presented with.
As the availability of volunteers is limited, a properly executed in-person evaluation is likely to get more information out of the participants, by asking questions and discussing potential issues with them.

A hybrid approach could also be used by performing an in-person evaluation, and requiring participants to fill in a questionnaire afterwards.
This would still require one or more pre-configured systems, and some time spent on each participant to give an introduction to the tool they are evaluating.
Doing this in an effective manner, requires multiple assistants, and while the meaning of the questions could be further explained upon request, the answers would still be limited in the same manner as with a web-based questionnaire.


%design science, se p√• it3010 forskningsmetoder i informatikk