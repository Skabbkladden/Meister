\chapter{Methodology}\label{methodology}
~\\
While exploring the field in the previous chapter, the first research question was answered in the process.
The next chapters will be used to answer the remaining questions using the following techniques.
~\\

Having looked into the various tools available, and compared their feature sets to determine which one is most likely to fit with the current teaching environment, question two is close to an answer as well.
By performing a closer study of the features of JIVE, its compatibility with the teaching environment can be answered to a better degree.
Such a study is also an essential part in finding whether there is any room for improvement, and answering the third question.
In addition to discovering potential improvements through the use and exploration of JIVE, determining whether the changes necessary to implement them are feasible to accomplish within the given time-frame is important to avoid half-implemented changes.
~\\

After thoroughly exploring JIVE, and identifying potential improvements, it is time to select what to improve, and implement the necessary changes.
The available time-frame limits how many improvements it is possible to implement, as some time will most certainly be lost in understanding the source code of JIVE.
Some time is also required at the end to answer the final research question.
This is best answered by performing a user evaluation with students in the target group.
This group consists of those described in the introduction, students in their second year of the computer science and informatics study-programs.
~\\

The types of evaluations available are limited to av few.
Reaching a wide audience, the use of questionnaires can provide a large amount of data to support the research, but the nature what is being evaluated makes this an infeasible method.
In order to give proper answers, the participants require a hands-on experience with the tool, and that would require them to acquire and install both the original version, and the modified version.
This is a rather large obstacle that would deter most potential candidates from participating, and defeats the point of a questionnaire.
The alternative is an in-person evaluation with a smaller group of students, where the participants get access to a pre-configured system and enough time to make an informed opinion of what they are presented with.
The availability of volunteers is not likely to be great in either case, but a properly executed in-person evaluation is likely to get more information out of the participants, by asking questions and discussing potential issues with them.


%design science, se p√• it3010 forskningsmetoder i informatikk